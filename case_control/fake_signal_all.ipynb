{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21762a84-4513-442f-8098-cfd947037865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import sklearn\n",
    "import os\n",
    "def set_seed(seed: int):\n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # If using multi-GPU.\n",
    "\n",
    "        # Ensure deterministic behavior in PyTorch (can slow down computations)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set sklearn random seed\n",
    "    sklearn.utils.check_random_state(seed)\n",
    "\n",
    "    # Set environment variable for reproducibility\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91414851-1492-40f8-9b35-861be06fc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read_h5ad(\"./data/t1d.h5ad\")\n",
    "print(adata)\n",
    "print(adata.X[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07920f87-be19-480b-85af-7d52ca0387fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000)\n",
    "adata = adata[:, adata.var[\"highly_variable\"]]\n",
    "sc.tl.pca(adata, svd_solver=\"arpack\")\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata,color=[\"cell_label\",\"disease_state\",\"cell_type\",\"donor_id\"],ncols=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941584f2-a49a-4dd8-9a9d-27e2499a8f4d",
   "metadata": {},
   "source": [
    "# If we randomly permute the donor_id for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4bbd9-b4a8-444d-9962-db07d8531247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "perm_indices = np.random.permutation(adata.obs.index)\n",
    "df = pd.DataFrame(adata.obs[['disease_state','donor_id']].copy())\n",
    "df_permuted=df.loc[perm_indices, ['disease_state','donor_id']].reset_index(drop=True)\n",
    "adata.obs['disease_state_permute']=df_permuted.loc[:,'disease_state'].values\n",
    "adata.obs['donor_id_permute']=df_permuted.loc[:,'donor_id'].values\n",
    "sc.pl.umap(adata,color=[\"cell_label\",\"disease_state\",\"cell_type\",\"donor_id\",'disease_state_permute','donor_id_permute'],ncols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9c28e-864b-499c-97cd-50d7db6640be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ndreamer import NDreamer\n",
    "\n",
    "model = NDreamer(adata, condition_key=\"disease_state_permute\", contorl_name='Control', num_hvg=2000, require_batch=True,\n",
    "                            batch_key='donor_id_permute',\n",
    "                            resolution_low=0.5, resolution_high=7, cluster_method=\"Leiden\", embedding_dim=512,\n",
    "                            codebooks=[1024 for i in range(32)],\n",
    "                            codebook_dim=8, encoder_hidden=[1024, 512], decoder_hidden=[512, 1024], z_dim=256,\n",
    "                            cos_loss_scaler=20, random_seed=123, batch_size=2048, epoches=10, lr=1e-3,\n",
    "                            triplet_margin=5,independent_loss_scaler=1000, save_pth=\"./t1d_fake_all/\",\n",
    "                            developer_test_mode=False,\n",
    "                            library_size_normalize_adata=False,\n",
    "                            save_preprocessed_adata_path=\"./t1d_fake_all/preprocessed.h5ad\",\n",
    "                            KL_scaler=5e-3, reconstruct_scaler=1, triplet_scaler=5, num_triplets_per_label=15,\n",
    "                            tau=0.01, commitment_loss_scaler=1, cluster_correlation_scaler=50,reset_threshold=1/1024,\n",
    "                            reset_interval=30,try_identify_cb_specific_subtypes=False,\n",
    "                            local_neighborhood_loss_scaler=1,local_neighbor_sigma=1,\n",
    "                            try_identify_perturb_escaped_cell=False,n_neighbors=20,\n",
    "                            local_neighbor_across_cluster_scaler=20)\n",
    "\n",
    "model.train_model()\n",
    "model.get_modifier_space()\n",
    "\n",
    "model.decompose_true_expression_batch_effect_all(nearest_neighbor=1,bandwidth=1)\n",
    "model.Estmiate_ITE_all(nearest_neighbor=1,bandwidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fe467-beb0-486c-b0c0-81a323d1780c",
   "metadata": {},
   "source": [
    "# Now we evaluate these two fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d26f0-a2b3-4770-8734-c3432081e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import gc\n",
    "import sys\n",
    "import cellanova as cnova\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea\n",
    "import os\n",
    "\n",
    "from metrics import calculate_metrics\n",
    "\n",
    "def calculate_mean_proportion_matrix(df):\n",
    "    \"\"\"\n",
    "    Calculates the mean proportion for each combination of condition and neighbor\n",
    "    and summarizes the result in a square matrix dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with columns ['condition', 'neighbor', 'proportion'].\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A square matrix dataframe where rows represent 'condition' and columns represent 'neighbor'.\n",
    "    \"\"\"\n",
    "    # Use a pivot table to calculate the mean proportions\n",
    "    mean_matrix = df.pivot_table(\n",
    "        index='condition',\n",
    "        columns='neighbor',\n",
    "        values='proportion',\n",
    "        aggfunc='mean',\n",
    "        fill_value=0  # Replace NaN with 0 if there are missing combinations\n",
    "    )\n",
    "\n",
    "    return mean_matrix\n",
    "\n",
    "def calculate_rowwise_correlation(adata1, adata2, batch_key=\"batch_all_with_condition\"):\n",
    "    # Ensure the obs index and batch_key match\n",
    "    #assert np.sum(adata1.obs[\"batch_all_with_condition\"]!=adata2.obs[\"batch_all_with_condition\"])==0, \"obs indices do not match between the two AnnData objects\"\n",
    "    assert batch_key in adata1.obs.columns, f\"{batch_key} not found in adata1.obs\"\n",
    "    assert batch_key in adata2.obs.columns, f\"{batch_key} not found in adata2.obs\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate through unique batches\n",
    "    unique_batches = adata1.obs[batch_key].unique()\n",
    "    for batch in unique_batches:\n",
    "        # Subset the data for the current batch\n",
    "        batch_mask = adata1.obs[batch_key] == batch\n",
    "        data1 = adata1[batch_mask].X\n",
    "        data2 = adata2[batch_mask].X\n",
    "        \n",
    "        barcodes=adata1[batch_mask].obs_names.tolist()\n",
    "\n",
    "        # Ensure the data is in dense format if sparse\n",
    "        if not isinstance(data1, np.ndarray):\n",
    "            data1 = data1.toarray()\n",
    "        if not isinstance(data2, np.ndarray):\n",
    "            data2 = data2.toarray()\n",
    "\n",
    "        # Compute correlation for each row\n",
    "        for i in range(data1.shape[0]):\n",
    "            row_corr = np.corrcoef(data1[i, :], data2[i, :])[0, 1]\n",
    "            mse=np.mean(np.square(data1[i, :]-data2[i, :]))\n",
    "            results.append({\"correlation\": row_corr, batch_key: batch, \"barcode\":barcodes[i], \"mse\":mse})\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def extract_de_results(adata, batch_key, cell_type_key, key_added=\"DE_wilcoxon\", min_cells=30):\n",
    "    results = []\n",
    "\n",
    "    # Iterate through unique batches\n",
    "    unique_batches = adata.obs[batch_key].unique()\n",
    "    for batch in unique_batches:\n",
    "        # Subset the AnnData for the current batch\n",
    "        adata_batch = adata[adata.obs[batch_key] == batch]\n",
    "\n",
    "        # Filter out cell types with fewer than `min_cells` cells\n",
    "        cell_counts = adata_batch.obs[cell_type_key].value_counts()\n",
    "        valid_cell_types = cell_counts[cell_counts >= min_cells].index\n",
    "\n",
    "        # Subset the AnnData object to only include valid cell types\n",
    "        adata_batch = adata_batch[adata_batch.obs[cell_type_key].isin(valid_cell_types)]\n",
    "\n",
    "        # Skip the batch if there are no valid cell types\n",
    "        if adata_batch.shape[0] == 0 or len(valid_cell_types) == 0:\n",
    "            continue\n",
    "\n",
    "        # Perform DE analysis\n",
    "        sc.tl.rank_genes_groups(adata_batch, groupby=cell_type_key, method='wilcoxon', key_added=key_added, use_raw=False)\n",
    "\n",
    "        # Extract DE results for each valid cell type\n",
    "        for cell_type in valid_cell_types:\n",
    "            # Extract adjusted p-values and gene names\n",
    "            gene_names = adata_batch.uns[key_added]['names'][cell_type]\n",
    "            pvals_adj = adata_batch.uns[key_added]['pvals_adj'][cell_type]\n",
    "\n",
    "            # Create a dictionary for the current cell type and batch\n",
    "            row_data = {\n",
    "                \"cell_type_key\": cell_type,\n",
    "                \"batch_all_with_condition\": batch,\n",
    "            }\n",
    "            # Add adjusted p-values for each gene as separate columns\n",
    "            row_data.update({gene: pval for gene, pval in zip(gene_names, pvals_adj)})\n",
    "            results.append(row_data)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902ac98-f423-4e6b-900c-038047f9a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ndreamer_batch(dataset_name, cell_type_key, batch_key, condition_key):\n",
    "    print(\"adata preprocessing...\")\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    adata = sc.read_h5ad(\"./\" + dataset_name + \"/adata.h5ad\")\n",
    "    adata.raw = None\n",
    "\n",
    "    if isinstance(batch_key, str):\n",
    "        batch_key = [batch_key]\n",
    "\n",
    "    batch_all = []\n",
    "    for i in range(adata.shape[0]):\n",
    "        tmp = \"__\".join([adata.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        batch_all.append(tmp)\n",
    "    batch_all = np.array(batch_all)\n",
    "    adata.obs[\"batch_all\"] = batch_all\n",
    "    adata.obs[\"batch_all\"] = adata.obs[\"batch_all\"].astype(\"category\")\n",
    "    print(\"batch_all\", np.unique(batch_all))\n",
    "\n",
    "    batch_all_with_condition = []\n",
    "    for i in range(adata.shape[0]):\n",
    "        tmp = \"__\".join([adata.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        tmp = tmp + \"__\" + adata.obs[condition_key][i]\n",
    "        batch_all_with_condition.append(tmp)\n",
    "    batch_all_with_condition = np.array(batch_all_with_condition)\n",
    "    adata.obs[\"batch_all_with_condition\"] = batch_all_with_condition\n",
    "    adata.obs[\"batch_all_with_condition\"] = adata.obs[\"batch_all_with_condition\"].astype(\"category\")\n",
    "    print(\"batch_all_with_condition\", np.unique(batch_all_with_condition))\n",
    "    \n",
    "    main_effect_adata = ad.AnnData(adata.obsm['X_effect_modifier_space_PCA'], dtype=np.float32)\n",
    "    main_effect_adata.obs = adata.obs.copy()\n",
    "\n",
    "    integrated = []\n",
    "    for filei in os.listdir(\"./\" + dataset_name):\n",
    "        if filei.find(\"expression.h5ad\") > 0:\n",
    "            integrated.append(sc.read_h5ad(\"./\" + dataset_name + \"/\" + filei))\n",
    "    integrated = ad.concat(integrated, merge=\"same\", uns_merge=\"same\")\n",
    "\n",
    "    # Process `batch_all` for denoised data\n",
    "    batch_all_denoise = []\n",
    "    for i in range(integrated.shape[0]):\n",
    "        tmp = \"__\".join([integrated.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        batch_all_denoise.append(tmp)\n",
    "    batch_all_denoise = np.array(batch_all_denoise)\n",
    "    integrated.obs[\"batch_all\"] = batch_all_denoise\n",
    "    integrated.obs[\"batch_all\"] = integrated.obs[\"batch_all\"].astype(\"category\")\n",
    "    print(\"batch_all_denoise\", np.unique(batch_all_denoise))\n",
    "\n",
    "    # Process `batch_all_with_condition` for denoised data\n",
    "    batch_all_with_condition_denoise = []\n",
    "    for i in range(integrated.shape[0]):\n",
    "        tmp = \"__\".join([integrated.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        tmp = tmp + \"__\" + integrated.obs[condition_key][i]\n",
    "        batch_all_with_condition_denoise.append(tmp)\n",
    "    batch_all_with_condition_denoise = np.array(batch_all_with_condition_denoise)\n",
    "    integrated.obs[\"batch_all_with_condition\"] = batch_all_with_condition_denoise\n",
    "    integrated.obs[\"batch_all_with_condition\"] = integrated.obs[\"batch_all_with_condition\"].astype(\"category\")\n",
    "    print(\"batch_all_with_condition_denoise\", np.unique(batch_all_with_condition_denoise))\n",
    "\n",
    "    batch_key.append(\"batch_all\")\n",
    "    batch_key.append(\"batch_all_with_condition\")\n",
    "    print(\"Finish preprocess\")\n",
    "\n",
    "    print(\"Calculating global distortion...\")\n",
    "    df_global_correlation = calculate_rowwise_correlation(adata, integrated)\n",
    "    df_global_correlation.to_csv(\"./evaluate/\" + dataset_name + \"_global_correlation.csv\")\n",
    "    print(\"Finish\")\n",
    "\n",
    "    print(\"Evaluate gene-level signal distortion\")\n",
    "    real_data_DE = extract_de_results(adata=adata, batch_key=\"batch_all_with_condition\", cell_type_key=cell_type_key,\n",
    "                                      key_added=\"DE_wilcoxon\")\n",
    "    denoised_DE = extract_de_results(adata=integrated, batch_key=\"batch_all_with_condition\",\n",
    "                                     cell_type_key=cell_type_key, key_added=\"DE_wilcoxon\")\n",
    "    real_data_DE.to_csv(\"./evaluate/\" + dataset_name + \"_real_DE.csv\")\n",
    "    denoised_DE.to_csv(\"./evaluate/\" + dataset_name + \"_denoised_DE.csv\")\n",
    "    print(\"Finish\")\n",
    "\n",
    "    '''res = cnova.utils.calc_oobNN(integrated, batch_key=\"batch_all_with_condition\", condition_key=condition_key)\n",
    "    df = res.obsm['knn_prop']\n",
    "    df['condition'] = res.obs[condition_key]\n",
    "    df.index.name = \"index\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = pd.melt(df, id_vars=['index', 'condition'], var_name='neighbor', value_name='proportion')\n",
    "    df = df.rename(columns={'index': 'obs_name'})\n",
    "    df.to_csv(\"./evaluate/\" + dataset_name + \"_oobNN.csv\")'''\n",
    "    \n",
    "    print(\"Plot of main effect:\")\n",
    "    if main_effect_adata.shape[1] > 60:\n",
    "        sc.pp.pca(main_effect_adata)\n",
    "    else:\n",
    "        main_effect_adata.obsm['X_pca'] = main_effect_adata.X.copy()\n",
    "    sc.pp.neighbors(main_effect_adata, n_neighbors=15)\n",
    "    sc.tl.umap(main_effect_adata)\n",
    "    for colori in [cell_type_key, condition_key] + batch_key:\n",
    "        sc.pl.umap(main_effect_adata, color=colori, ncols=1)\n",
    "\n",
    "    print(\"Plot of denoised expression\")\n",
    "    sc.pp.pca(integrated)\n",
    "    sc.pp.neighbors(integrated, n_neighbors=15)\n",
    "    sc.tl.umap(integrated)\n",
    "    for colori in [cell_type_key, condition_key] + batch_key:\n",
    "        sc.pl.umap(integrated, color=colori, ncols=1)\n",
    "    print(\"Finish adata preprocessing\", \"=\" * 20)\n",
    "    \n",
    "    print(\"Batch effect:\")\n",
    "    print(\"condition-related mixing performance evaluation for main effect:\")\n",
    "    import rpy2.robjects as robjects\n",
    "    import anndata2ri\n",
    "    anndata2ri.activate()\n",
    "    # Add your library path\n",
    "    library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "    # Update R's library paths\n",
    "    robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "    # Verify the updated library paths\n",
    "    print(robjects.r('.libPaths()'))\n",
    "    calculate_metrics(main_effect_adata, batch_key=condition_key,\n",
    "                      celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    print(\"batch-related mixing performance evaluation for main effect:\")\n",
    "    import rpy2.robjects as robjects\n",
    "    import anndata2ri\n",
    "    anndata2ri.activate()\n",
    "    # Add your library path\n",
    "    library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "    # Update R's library paths\n",
    "    robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "    # Verify the updated library paths\n",
    "    print(robjects.r('.libPaths()'))\n",
    "    calculate_metrics(main_effect_adata, batch_key=\"batch_all\",\n",
    "                      celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    print(\"batch-condition-related mixing performance evaluation for main effect:\")\n",
    "    import rpy2.robjects as robjects\n",
    "    import anndata2ri\n",
    "    anndata2ri.activate()\n",
    "    # Add your library path\n",
    "    library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "    # Update R's library paths\n",
    "    robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "    # Verify the updated library paths\n",
    "    print(robjects.r('.libPaths()'))\n",
    "    calculate_metrics(main_effect_adata, batch_key=\"batch_all_with_condition\",\n",
    "                      celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    print(\"Within each unique condition, evaluate the batch effect of the denoised expression\")\n",
    "    unique_conditions=np.unique(np.array(integrated.obs[condition_key]))\n",
    "    for conditioni in unique_conditions:\n",
    "        print(\"Within each unique condition, evaluate the batch effect of the denoised expression\")\n",
    "        print(\"Now evaluating\",conditioni)\n",
    "        integratedi=integrated[integrated.obs[condition_key]==conditioni]\n",
    "        import rpy2.robjects as robjects\n",
    "        import anndata2ri\n",
    "        anndata2ri.activate()\n",
    "        # Add your library path\n",
    "        library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "        # Update R's library paths\n",
    "        robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "        # Verify the updated library paths\n",
    "        print(robjects.r('.libPaths()'))\n",
    "        calculate_metrics(integratedi, batch_key=\"batch_all\",\n",
    "                          celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        for colori in [\"batch_all\",cell_type_key]:\n",
    "            sc.pl.umap(integratedi, color=colori, ncols=1)\n",
    "        print(\"*\"*20)\n",
    "\n",
    "    print(\"Batch-related mixing performance evaluation for main effect:\")\n",
    "    '''\n",
    "    for batch_keyi in batch_key:\n",
    "        print(\"Batch-related mixing performance evaluation for main effect:\")\n",
    "        print(batch_keyi)\n",
    "        if batch_keyi.find(\".\")>=0:\n",
    "            main_effect_adata.obs[batch_keyi.replace(\".\",\"_\")]=main_effect_adata.obs[batch_keyi].copy()\n",
    "            batch_keyi=batch_keyi.replace(\".\",\"_\")\n",
    "        import rpy2.robjects as robjects\n",
    "        import anndata2ri\n",
    "        anndata2ri.activate()\n",
    "        # Add your library path\n",
    "        library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "        # Update R's library paths\n",
    "        robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "        # Verify the updated library paths\n",
    "        print(robjects.r('.libPaths()'))\n",
    "        calculate_metrics(main_effect_adata, batch_key=batch_keyi,\n",
    "                          celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        print(\"=\" * 20)\n",
    "    '''\n",
    "    res = cnova.utils.calc_oobNN(integrated, batch_key=\"batch_all_with_condition\", condition_key=condition_key)\n",
    "    df = res.obsm['knn_prop']\n",
    "    df['condition'] = res.obs[condition_key]\n",
    "    df.index.name = \"index\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = pd.melt(df, id_vars=['index', 'condition'], var_name='neighbor', value_name='proportion')\n",
    "    df = df.rename(columns={'index': 'obs_name'})\n",
    "    df.to_csv(\"./evaluate/\" + dataset_name + \"_oobNN.csv\")\n",
    "\n",
    "    g = sea.FacetGrid(df, col='neighbor', hue='condition')\n",
    "    g.map(sea.kdeplot, 'proportion', bw_adjust=2, alpha=1)\n",
    "    g.set(xlabel='NN proportion', ylabel='Density')\n",
    "    g.add_legend()\n",
    "    plt.suptitle('NDreamer integration')\n",
    "    sea.set_style('white')\n",
    "    plt.show()\n",
    "\n",
    "    df_summarize = calculate_mean_proportion_matrix(df)\n",
    "    print(df_summarize)\n",
    "    df_summarize.to_csv(\"./evaluate/\" + dataset_name + \"_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f07abb-3fc0-4505-9527-00cd386d677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[\"t1d_fake\",\"t1d_fake_all\"]\n",
    "cell_type_keys=[\"cell_type\",\"cell_type\"]\n",
    "batch_keys=[\"donor_id\",\"donor_id_permute\"]\n",
    "condition_keys=[\"disease_state_fake\",\"disease_state_permute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507db98-bb57-4267-8533-6e68dde39e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "evaluate_ndreamer_batch(dataset_name=dataset_names[i],cell_type_key=cell_type_keys[i],batch_key=batch_keys[i],condition_key=condition_keys[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
