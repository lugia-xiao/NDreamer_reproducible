{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06b01a-5945-49b1-99b0-0c7217566134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import gc\n",
    "import sys\n",
    "import cellanova as cnova\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea\n",
    "\n",
    "from metrics import calculate_metrics\n",
    "\n",
    "sc.settings.verbosity = 0\n",
    "sc.settings.set_figure_params(dpi=400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "seed = 10\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc609030-361d-46cf-8754-717f93f6fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_proportion_matrix(df):\n",
    "    \"\"\"\n",
    "    Calculates the mean proportion for each combination of condition and neighbor\n",
    "    and summarizes the result in a square matrix dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with columns ['condition', 'neighbor', 'proportion'].\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A square matrix dataframe where rows represent 'condition' and columns represent 'neighbor'.\n",
    "    \"\"\"\n",
    "    # Use a pivot table to calculate the mean proportions\n",
    "    mean_matrix = df.pivot_table(\n",
    "        index='condition',\n",
    "        columns='neighbor',\n",
    "        values='proportion',\n",
    "        aggfunc='mean',\n",
    "        fill_value=0  # Replace NaN with 0 if there are missing combinations\n",
    "    )\n",
    "\n",
    "    return mean_matrix\n",
    "\n",
    "def calculate_rowwise_correlation(adata1, adata2, batch_key=\"batch_all_with_condition\"):\n",
    "    # Ensure the obs index and batch_key match\n",
    "    #assert np.sum(adata1.obs[\"batch_all_with_condition\"]!=adata2.obs[\"batch_all_with_condition\"])==0, \"obs indices do not match between the two AnnData objects\"\n",
    "    assert batch_key in adata1.obs.columns, f\"{batch_key} not found in adata1.obs\"\n",
    "    assert batch_key in adata2.obs.columns, f\"{batch_key} not found in adata2.obs\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate through unique batches\n",
    "    unique_batches = adata1.obs[batch_key].unique()\n",
    "    for batch in unique_batches:\n",
    "        # Subset the data for the current batch\n",
    "        batch_mask = adata1.obs[batch_key] == batch\n",
    "        data1 = adata1[batch_mask].X\n",
    "        data2 = adata2[batch_mask].X\n",
    "        \n",
    "        barcodes=adata1[batch_mask].obs_names.tolist()\n",
    "\n",
    "        # Ensure the data is in dense format if sparse\n",
    "        if not isinstance(data1, np.ndarray):\n",
    "            data1 = data1.toarray()\n",
    "        if not isinstance(data2, np.ndarray):\n",
    "            data2 = data2.toarray()\n",
    "\n",
    "        # Compute correlation for each row\n",
    "        for i in range(data1.shape[0]):\n",
    "            row_corr = np.corrcoef(data1[i, :], data2[i, :])[0, 1]\n",
    "            mse=np.mean(np.square(data1[i, :]-data2[i, :]))\n",
    "            results.append({\"correlation\": row_corr, batch_key: batch, \"barcode\":barcodes[i], \"mse\":mse})\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df\n",
    "\n",
    "def extract_de_results(adata, batch_key, cell_type_key, key_added=\"DE_wilcoxon\", min_cells=30):\n",
    "    results = []\n",
    "\n",
    "    # Iterate through unique batches\n",
    "    unique_batches = adata.obs[batch_key].unique()\n",
    "    for batch in unique_batches:\n",
    "        # Subset the AnnData for the current batch\n",
    "        adata_batch = adata[adata.obs[batch_key] == batch]\n",
    "        \n",
    "        # Filter out cell types with fewer than `min_cells` cells\n",
    "        cell_counts = adata_batch.obs[cell_type_key].value_counts()\n",
    "        valid_cell_types = cell_counts[cell_counts >= min_cells].index\n",
    "        \n",
    "        # Subset the AnnData object to only include valid cell types\n",
    "        adata_batch = adata_batch[adata_batch.obs[cell_type_key].isin(valid_cell_types)]\n",
    "        \n",
    "        # Skip the batch if there are no valid cell types\n",
    "        if adata_batch.shape[0] == 0 or len(valid_cell_types) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Perform DE analysis\n",
    "        sc.tl.rank_genes_groups(adata_batch, groupby=cell_type_key, method='wilcoxon', key_added=key_added, use_raw=False)\n",
    "        \n",
    "        # Extract DE results for each valid cell type\n",
    "        for cell_type in valid_cell_types:\n",
    "            # Extract adjusted p-values and gene names\n",
    "            gene_names = adata_batch.uns[key_added]['names'][cell_type]\n",
    "            pvals_adj = adata_batch.uns[key_added]['pvals_adj'][cell_type]\n",
    "            \n",
    "            # Create a dictionary for the current cell type and batch\n",
    "            row_data = {\n",
    "                \"cell_type_key\": cell_type,\n",
    "                \"batch_all_with_condition\": batch,\n",
    "            }\n",
    "            # Add adjusted p-values for each gene as separate columns\n",
    "            row_data.update({gene: pval for gene, pval in zip(gene_names, pvals_adj)})\n",
    "            results.append(row_data)\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df\n",
    "\n",
    "def evaluate_scd_comprehensive(adata, cell_type_key, batch_key, condition_key, dataset_name):\n",
    "    print(dataset_name, \"adata preprocessing...\")\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    adata.raw = None\n",
    "    print(\"adata preprocessing...\")\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    if isinstance(batch_key, str):\n",
    "        batch_key = [batch_key]\n",
    "\n",
    "    batch_all = []\n",
    "    for i in range(adata.shape[0]):\n",
    "        tmp = \"__\".join([adata.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        batch_all.append(tmp)\n",
    "    batch_all = np.array(batch_all)\n",
    "    adata.obs[\"batch_all\"] = batch_all\n",
    "    adata.obs[\"batch_all\"] = adata.obs[\"batch_all\"].astype(\"category\")\n",
    "    print(\"batch_all\", np.unique(batch_all))\n",
    "\n",
    "    batch_all_with_condition = []\n",
    "    for i in range(adata.shape[0]):\n",
    "        tmp = \"__\".join([adata.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        tmp = tmp + \"__\" + adata.obs[condition_key][i]\n",
    "        batch_all_with_condition.append(tmp)\n",
    "    batch_all_with_condition = np.array(batch_all_with_condition)\n",
    "    adata.obs[\"batch_all_with_condition\"] = batch_all_with_condition\n",
    "    adata.obs[\"batch_all_with_condition\"] = adata.obs[\"batch_all_with_condition\"].astype(\"category\")\n",
    "    print(\"batch_all_with_condition\", np.unique(batch_all_with_condition))\n",
    "\n",
    "    batch_key.append(\"batch_all\")\n",
    "    batch_key.append(\"batch_all_with_condition\")\n",
    "\n",
    "    main_effect_adata = ad.AnnData(adata.obsm['main_effect'], dtype=np.float32)\n",
    "    #main_effect_adata.var_names = adata.var_names\n",
    "    main_effect_adata.obs = adata.obs.copy()\n",
    "\n",
    "    integrated = ad.AnnData(adata.layers['denoised'], dtype=np.float32)\n",
    "    integrated.obs = adata.obs.copy()\n",
    "    integrated.var_names = adata.var_names\n",
    "\n",
    "    batch_key.append(\"batch_all\")\n",
    "    batch_key.append(\"batch_all_with_condition\")\n",
    "    print(\"Finish preprocess\")\n",
    "\n",
    "    print(dataset_name, \"Plot of main effect:\")\n",
    "    if main_effect_adata.shape[1] > 60:\n",
    "        sc.pp.pca(main_effect_adata)\n",
    "    else:\n",
    "        main_effect_adata.obsm['X_pca'] = main_effect_adata.X.copy()\n",
    "    sc.pp.neighbors(main_effect_adata, n_neighbors=15)\n",
    "    sc.tl.umap(main_effect_adata)\n",
    "    sc.pl.umap(main_effect_adata, color=[cell_type_key, condition_key] + batch_key, ncols=1)\n",
    "\n",
    "    print(dataset_name, \"Plot of denoised expression\")\n",
    "    sc.pp.pca(integrated)\n",
    "    sc.pp.neighbors(integrated, n_neighbors=15)\n",
    "    sc.tl.umap(integrated)\n",
    "    sc.pl.umap(integrated, color=[cell_type_key, condition_key] + batch_key, ncols=1)\n",
    "\n",
    "    print(\"Calculating global distortion...\")\n",
    "    df_global_correlation=calculate_rowwise_correlation(adata, integrated)\n",
    "    df_global_correlation.to_csv(\"./scd/\"+dataset_name+\"_global_correlation.csv\")\n",
    "    print(\"Finish\")\n",
    "\n",
    "    print(\"Evaluate gene-level signal distortion\")\n",
    "    if True:#dataset_name!=\"mouse\":\n",
    "        real_data_DE = extract_de_results(adata=adata, batch_key=\"batch_all_with_condition\", cell_type_key=cell_type_key,key_added=\"DE_wilcoxon\")\n",
    "        denoised_DE = extract_de_results(adata=integrated, batch_key=\"batch_all_with_condition\", cell_type_key=cell_type_key,key_added=\"DE_wilcoxon\")\n",
    "        real_data_DE.to_csv(\"./scd/\"+dataset_name+\"_real_DE.csv\")\n",
    "        denoised_DE.to_csv(\"./scd/\"+dataset_name+\"_denoised_DE.csv\")\n",
    "    print(\"Finish\")\n",
    "    \n",
    "    print(dataset_name, \"Batch-related mixing performance evaluation for main effect:\")\n",
    "    for batch_keyi in batch_key + [condition_key]:\n",
    "        print(dataset_name, \"Batch-related mixing performance evaluation for main effect:\")\n",
    "        print(batch_keyi)\n",
    "        import rpy2.robjects as robjects\n",
    "        import anndata2ri\n",
    "        anndata2ri.activate()\n",
    "        # Add your library path\n",
    "        library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "        # Update R's library paths\n",
    "        robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "        # Verify the updated library paths\n",
    "        print(robjects.r('.libPaths()'))\n",
    "        calculate_metrics(main_effect_adata, batch_key=batch_keyi,\n",
    "                          celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "    print(dataset_name, \"Within each unique condition, plot main effect\")\n",
    "    unique_conditions = np.unique(np.array(integrated.obs[condition_key]))\n",
    "    for conditioni in unique_conditions:\n",
    "        print(dataset_name, \"Now evaluating\", conditioni, \"main effect\")\n",
    "        main_effect_adatai = main_effect_adata[main_effect_adata.obs[condition_key] == conditioni]\n",
    "        sc.pl.umap(main_effect_adatai, color=[cell_type_key] + batch_key, ncols=1)\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    print(dataset_name, \"NMI, ARI for condition in denoised expression\")\n",
    "    import rpy2.robjects as robjects\n",
    "    import anndata2ri\n",
    "    anndata2ri.activate()\n",
    "    # Add your library path\n",
    "    library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "    # Update R's library paths\n",
    "    robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "    # Verify the updated library paths\n",
    "    print(robjects.r('.libPaths()'))\n",
    "    calculate_metrics(integrated, batch_key=\"batch_all\",\n",
    "                      celltype_key=condition_key, all=True, n_neighbors=15)\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    print(dataset_name, \"Within each unique condition, plot denoised expression\")\n",
    "    unique_conditions = np.unique(np.array(integrated.obs[condition_key]))\n",
    "    for conditioni in unique_conditions:\n",
    "        print(dataset_name, \"Now evaluating\", conditioni, \"denoised expression\")\n",
    "        integratedi = integrated[integrated.obs[condition_key] == conditioni]\n",
    "        sc.pl.umap(integratedi, color=[cell_type_key] + batch_key, ncols=1)\n",
    "        import rpy2.robjects as robjects\n",
    "        import anndata2ri\n",
    "        anndata2ri.activate()\n",
    "        # Add your library path\n",
    "        library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "        # Update R's library paths\n",
    "        robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "        # Verify the updated library paths\n",
    "        print(robjects.r('.libPaths()'))\n",
    "        calculate_metrics(integratedi, batch_key=\"batch_all\",\n",
    "                          celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        print(\"*\" * 20)\n",
    "\n",
    "    print(\"Neighborhood analysis:\",dataset_name)\n",
    "    res = cnova.utils.calc_oobNN(integrated, batch_key=\"batch_all_with_condition\", condition_key=condition_key)\n",
    "    df = res.obsm['knn_prop']\n",
    "    df['condition'] = res.obs[condition_key]\n",
    "    df.index.name = \"index\"\n",
    "    df = df.reset_index()\n",
    "    df = pd.melt(df, id_vars=['index', 'condition'], var_name='neighbor', value_name='proportion')\n",
    "    df = df.rename(columns={'index': 'obs_name'})\n",
    "    df.to_csv(\"./scd/\" + dataset_name + \"_oobNN.csv\")\n",
    "\n",
    "    g = sea.FacetGrid(df, col='neighbor', hue='condition')\n",
    "    g.map(sea.kdeplot, 'proportion', bw_adjust=2, alpha=1)\n",
    "    g.set(xlabel='NN proportion', ylabel='Density')\n",
    "    g.add_legend()\n",
    "    plt.suptitle('scd integration')\n",
    "    sea.set_style('white')\n",
    "    plt.show()\n",
    "\n",
    "    df_summarize = calculate_mean_proportion_matrix(df)\n",
    "    print(df_summarize)\n",
    "    df_summarize.to_csv(\"./scd/\" + dataset_name + \"_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587366d-bd0f-4e33-b963-fffd28a7b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECCITE\n",
    "adata=sc.read_h5ad(\"./scd/\" + \"ECCITE\" + \"_latent.h5ad\")\n",
    "evaluate_scd_comprehensive(adata=adata, cell_type_key=\"Phase\", batch_key='replicate', condition_key='perturbation', dataset_name=\"ECCITE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74869271-3c73-4d7b-a16a-4d4821868ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASD\n",
    "adata=sc.read_h5ad(\"./scd/ASD_latent.h5ad\")\n",
    "evaluate_scd_comprehensive(adata=adata, cell_type_key=\"CellType\", batch_key='Batch', condition_key='perturb01', dataset_name=\"ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb0ac3-e1ba-42ac-8f99-433d376c64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASD1\n",
    "adata=sc.read_h5ad(\"./scd/ASD1_latent.h5ad\")\n",
    "evaluate_scd_comprehensive(adata=adata, cell_type_key=\"CellType\", batch_key='Batch', condition_key='perturb01', dataset_name=\"ASD1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
