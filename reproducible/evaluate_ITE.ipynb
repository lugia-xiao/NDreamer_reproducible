{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd9dca6-9a05-407f-bb64-449fe801ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to determine R library path: Command '('/vast/palmer/apps/avx2/software/R/4.3.2-foss-2022b-patched/lib64/R/bin/Rscript', '-e', 'cat(Sys.getenv(\"LD_LIBRARY_PATH\"))')' returned non-zero exit status 127.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_HOME is already set to: /vast/palmer/apps/avx2/software/R/4.3.2-foss-2022b-patched/lib64/R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/wang_zuoheng/xx244/Ndreamer/reproducible/metrics.py:29: DeprecationWarning: The global conversion available with activate() is deprecated and will be removed in the next major release. Use a local converter.\n",
      "  anndata2ri.activate()\n",
      "/home/xx244/.conda/envs/benchmark/lib/python3.11/site-packages/rpy2/robjects/pandas2ri.py:368: DeprecationWarning: The global conversion available with activate() is deprecated and will be removed in the next major release. Use a local converter.\n",
      "  warnings.warn('The global conversion available with activate() '\n",
      "/home/xx244/.conda/envs/benchmark/lib/python3.11/site-packages/rpy2/robjects/numpy2ri.py:241: DeprecationWarning: The global conversion available with activate() is deprecated and will be removed in the next major release. Use a local converter.\n",
      "  warnings.warn('The global conversion available with activate() '\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import gc\n",
    "import sys\n",
    "import cellanova as cnova\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea\n",
    "import os\n",
    "\n",
    "from metrics import calculate_metrics\n",
    "\n",
    "sc.settings.verbosity = 0\n",
    "sc.settings.set_figure_params(dpi=400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "seed = 10\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8002829d-fa2a-48a8-bc19-08000e20afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_proportion_matrix(df):\n",
    "    \"\"\"\n",
    "    Calculates the mean proportion for each combination of condition and neighbor\n",
    "    and summarizes the result in a square matrix dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with columns ['condition', 'neighbor', 'proportion'].\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A square matrix dataframe where rows represent 'condition' and columns represent 'neighbor'.\n",
    "    \"\"\"\n",
    "    # Use a pivot table to calculate the mean proportions\n",
    "    mean_matrix = df.pivot_table(\n",
    "        index='condition',\n",
    "        columns='neighbor',\n",
    "        values='proportion',\n",
    "        aggfunc='mean',\n",
    "        fill_value=0  # Replace NaN with 0 if there are missing combinations\n",
    "    )\n",
    "\n",
    "    return mean_matrix\n",
    "\n",
    "def calculate_rowwise_correlation(adata1, adata2, batch_key=\"batch_all_with_condition\"):\n",
    "    # Ensure the obs index and batch_key match\n",
    "    #assert np.sum(adata1.obs[\"batch_all_with_condition\"]!=adata2.obs[\"batch_all_with_condition\"])==0, \"obs indices do not match between the two AnnData objects\"\n",
    "    assert batch_key in adata1.obs.columns, f\"{batch_key} not found in adata1.obs\"\n",
    "    assert batch_key in adata2.obs.columns, f\"{batch_key} not found in adata2.obs\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Iterate through unique batches\n",
    "    unique_batches = adata1.obs[batch_key].unique()\n",
    "    for batch in unique_batches:\n",
    "        # Subset the data for the current batch\n",
    "        batch_mask = adata1.obs[batch_key] == batch\n",
    "        data1 = adata1[batch_mask].X\n",
    "        data2 = adata2[batch_mask].X\n",
    "        \n",
    "        barcodes=adata1[batch_mask].obs_names.tolist()\n",
    "\n",
    "        # Ensure the data is in dense format if sparse\n",
    "        if not isinstance(data1, np.ndarray):\n",
    "            data1 = data1.toarray()\n",
    "        if not isinstance(data2, np.ndarray):\n",
    "            data2 = data2.toarray()\n",
    "\n",
    "        # Compute correlation for each row\n",
    "        for i in range(data1.shape[0]):\n",
    "            row_corr = np.corrcoef(data1[i, :], data2[i, :])[0, 1]\n",
    "            mse=np.mean(np.square(data1[i, :]-data2[i, :]))\n",
    "            results.append({\"correlation\": row_corr, batch_key: batch, \"barcode\":barcodes[i], \"mse\":mse})\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def extract_de_results(adata, batch_key, cell_type_key, key_added=\"DE_wilcoxon\", min_cells=30):\n",
    "    results = []\n",
    "\n",
    "    # Iterate through unique batches\n",
    "    unique_batches = adata.obs[batch_key].unique()\n",
    "    for batch in unique_batches:\n",
    "        # Subset the AnnData for the current batch\n",
    "        adata_batch = adata[adata.obs[batch_key] == batch]\n",
    "\n",
    "        # Filter out cell types with fewer than `min_cells` cells\n",
    "        cell_counts = adata_batch.obs[cell_type_key].value_counts()\n",
    "        valid_cell_types = cell_counts[cell_counts >= min_cells].index\n",
    "\n",
    "        # Subset the AnnData object to only include valid cell types\n",
    "        adata_batch = adata_batch[adata_batch.obs[cell_type_key].isin(valid_cell_types)]\n",
    "\n",
    "        # Skip the batch if there are no valid cell types\n",
    "        if adata_batch.shape[0] == 0 or len(valid_cell_types) == 0:\n",
    "            continue\n",
    "\n",
    "        # Perform DE analysis\n",
    "        sc.tl.rank_genes_groups(adata_batch, groupby=cell_type_key, method='wilcoxon', key_added=key_added, use_raw=False)\n",
    "\n",
    "        # Extract DE results for each valid cell type\n",
    "        for cell_type in valid_cell_types:\n",
    "            # Extract adjusted p-values and gene names\n",
    "            gene_names = adata_batch.uns[key_added]['names'][cell_type]\n",
    "            pvals_adj = adata_batch.uns[key_added]['pvals_adj'][cell_type]\n",
    "\n",
    "            # Create a dictionary for the current cell type and batch\n",
    "            row_data = {\n",
    "                \"cell_type_key\": cell_type,\n",
    "                \"batch_all_with_condition\": batch,\n",
    "            }\n",
    "            # Add adjusted p-values for each gene as separate columns\n",
    "            row_data.update({gene: pval for gene, pval in zip(gene_names, pvals_adj)})\n",
    "            results.append(row_data)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabd8e52-2bc2-4385-9694-b5a432133edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ndreamer_ITE_batch(dataset_name, cell_type_key, batch_key, condition_key):\n",
    "    print(dataset_name, \"adata preprocessing...\")\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    adata = sc.read_h5ad(\"./\" + dataset_name + \"/adata.h5ad\")\n",
    "    adata.raw = None\n",
    "\n",
    "    if batch_key is None:\n",
    "        batch_key = condition_key\n",
    "    if isinstance(batch_key, str):\n",
    "        batch_key = [batch_key]\n",
    "\n",
    "    batch_all = []\n",
    "    for i in range(adata.shape[0]):\n",
    "        tmp = \"__\".join([adata.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        batch_all.append(tmp)\n",
    "    batch_all = np.array(batch_all)\n",
    "    adata.obs[\"batch_all\"] = batch_all\n",
    "    adata.obs[\"batch_all\"] = adata.obs[\"batch_all\"].astype(\"category\")\n",
    "    print(dataset_name, \"batch_all\", np.unique(batch_all))\n",
    "\n",
    "    batch_all_with_condition = []\n",
    "    for i in range(adata.shape[0]):\n",
    "        tmp = \"__\".join([adata.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        tmp = tmp + \"__\" + adata.obs[condition_key][i]\n",
    "        batch_all_with_condition.append(tmp)\n",
    "    batch_all_with_condition = np.array(batch_all_with_condition)\n",
    "    adata.obs[\"batch_all_with_condition\"] = batch_all_with_condition\n",
    "    adata.obs[\"batch_all_with_condition\"] = adata.obs[\"batch_all_with_condition\"].astype(\"category\")\n",
    "    print(dataset_name, \"batch_all_with_condition\", np.unique(batch_all_with_condition))\n",
    "\n",
    "    main_effect_adata = ad.AnnData(adata.obsm['X_effect_modifier_space_PCA'], dtype=np.float32)\n",
    "    main_effect_adata.obs = adata.obs.copy()\n",
    "\n",
    "    integrated = []\n",
    "    for filei in os.listdir(\"./\" + dataset_name):\n",
    "        if filei.find(\"expression.h5ad\") > 0:\n",
    "            integrated.append(sc.read_h5ad(\"./\" + dataset_name + \"/\" + filei))\n",
    "    integrated = ad.concat(integrated, merge=\"same\", uns_merge=\"same\")\n",
    "\n",
    "    # Process `batch_all` for denoised data\n",
    "    batch_all_denoise = []\n",
    "    for i in range(integrated.shape[0]):\n",
    "        tmp = \"__\".join([integrated.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        batch_all_denoise.append(tmp)\n",
    "    batch_all_denoise = np.array(batch_all_denoise)\n",
    "    integrated.obs[\"batch_all\"] = batch_all_denoise\n",
    "    integrated.obs[\"batch_all\"] = integrated.obs[\"batch_all\"].astype(\"category\")\n",
    "    print(dataset_name, \"batch_all_denoise\", np.unique(batch_all_denoise))\n",
    "\n",
    "    # Process `batch_all_with_condition` for denoised data\n",
    "    batch_all_with_condition_denoise = []\n",
    "    for i in range(integrated.shape[0]):\n",
    "        tmp = \"__\".join([integrated.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        tmp = tmp + \"__\" + integrated.obs[condition_key][i]\n",
    "        batch_all_with_condition_denoise.append(tmp)\n",
    "    batch_all_with_condition_denoise = np.array(batch_all_with_condition_denoise)\n",
    "    integrated.obs[\"batch_all_with_condition\"] = batch_all_with_condition_denoise\n",
    "    integrated.obs[\"batch_all_with_condition\"] = integrated.obs[\"batch_all_with_condition\"].astype(\"category\")\n",
    "    print(dataset_name, \"batch_all_with_condition_denoise\", np.unique(batch_all_with_condition_denoise))\n",
    "\n",
    "    # Handle `ITE` files\n",
    "    ITE = []\n",
    "    for filei in os.listdir(\"./\" + dataset_name):\n",
    "        if filei.find(\"ITE.h5ad\") > 0:\n",
    "            ITE.append(sc.read_h5ad(\"./\" + dataset_name + \"/\" + filei))\n",
    "    ITE = ad.concat(ITE, merge=\"same\", uns_merge=\"same\")\n",
    "\n",
    "    # Process `batch_all` for ITE data\n",
    "    batch_all_ite = []\n",
    "    for i in range(ITE.shape[0]):\n",
    "        tmp = \"__\".join([ITE.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        batch_all_ite.append(tmp)\n",
    "    batch_all_ite = np.array(batch_all_ite)\n",
    "    ITE.obs[\"batch_all\"] = batch_all_ite\n",
    "    ITE.obs[\"batch_all\"] = ITE.obs[\"batch_all\"].astype(\"category\")\n",
    "    print(dataset_name, \"batch_all_ite\", np.unique(batch_all_ite))\n",
    "\n",
    "    # Process `batch_all_with_condition` for ITE data\n",
    "    batch_all_with_condition_ite = []\n",
    "    for i in range(ITE.shape[0]):\n",
    "        tmp = \"__\".join([ITE.obs[batch_keyj][i] for batch_keyj in batch_key])\n",
    "        tmp = tmp + \"__\" + ITE.obs[condition_key][i]\n",
    "        batch_all_with_condition_ite.append(tmp)\n",
    "    batch_all_with_condition_ite = np.array(batch_all_with_condition_ite)\n",
    "    ITE.obs[\"batch_all_with_condition\"] = batch_all_with_condition_ite\n",
    "    ITE.obs[\"batch_all_with_condition\"] = ITE.obs[\"batch_all_with_condition\"].astype(\"category\")\n",
    "    print(dataset_name, \"batch_all_with_condition_ite\", np.unique(batch_all_with_condition_ite))\n",
    "\n",
    "    batch_key.append(\"batch_all\")\n",
    "    batch_key.append(\"batch_all_with_condition\")\n",
    "    print(\"Finish preprocess\")\n",
    "\n",
    "    '''print(dataset_name, \"Within each unique condition, plot ITE\")\n",
    "    sc.pp.pca(ITE)\n",
    "    sc.pp.neighbors(ITE, n_neighbors=15)\n",
    "    sc.tl.umap(ITE)\n",
    "    unique_conditions = np.unique(np.array(integrated.obs[condition_key]))\n",
    "    for conditioni in unique_conditions:\n",
    "        print(dataset_name, \"Now evaluating\", conditioni, \"ITE\")\n",
    "        if np.sum(ITE.obs[condition_key] == conditioni) == 0:\n",
    "            continue\n",
    "        ITEi = ITE[ITE.obs[condition_key] == conditioni]\n",
    "        print(ITEi.shape)\n",
    "        sc.pl.umap(ITEi, color=[cell_type_key] + batch_key, ncols=1)\n",
    "\n",
    "        for batch_keyi in batch_key + [condition_key]:\n",
    "            print(dataset_name, \"Batch-related mixing performance evaluation for ITE:\")\n",
    "            print(dataset_name, conditioni,batch_keyi)\n",
    "            import rpy2.robjects as robjects\n",
    "            import anndata2ri\n",
    "            anndata2ri.activate()\n",
    "            # Add your library path\n",
    "            library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "            # Update R's library paths\n",
    "            robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "            # Verify the updated library paths\n",
    "            print(robjects.r('.libPaths()'))\n",
    "            calculate_metrics(ITEi, batch_key=batch_keyi,\n",
    "                              celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        print(\"+\" * 20)'''\n",
    "\n",
    "    print(dataset_name, \"Plot of main effect:\")\n",
    "    if main_effect_adata.shape[1] > 60:\n",
    "        sc.pp.pca(main_effect_adata)\n",
    "    else:\n",
    "        main_effect_adata.obsm['X_pca'] = main_effect_adata.X.copy()\n",
    "    sc.pp.neighbors(main_effect_adata, n_neighbors=15)\n",
    "    sc.tl.umap(main_effect_adata)\n",
    "    sc.pl.umap(main_effect_adata, color=[cell_type_key, condition_key] + batch_key, ncols=1)\n",
    "\n",
    "    print(dataset_name, \"Plot of denoised expression\")\n",
    "    sc.pp.pca(integrated)\n",
    "    sc.pp.neighbors(integrated, n_neighbors=15)\n",
    "    sc.tl.umap(integrated)\n",
    "    sc.pl.umap(integrated, color=[cell_type_key, condition_key] + batch_key, ncols=1)\n",
    "\n",
    "    print(dataset_name, \"Plot of ITE\")\n",
    "    sc.pp.pca(ITE)\n",
    "    sc.pp.neighbors(ITE, n_neighbors=15)\n",
    "    sc.tl.umap(ITE)\n",
    "    sc.pl.umap(ITE, color=[cell_type_key, condition_key] + batch_key, ncols=1)\n",
    "\n",
    "    print(\"Calculating global distortion...\")\n",
    "    df_global_correlation=calculate_rowwise_correlation(adata, integrated)\n",
    "    df_global_correlation.to_csv(\"./evaluate/\"+dataset_name+\"_global_correlation.csv\")\n",
    "    print(\"Finish\")\n",
    "\n",
    "    print(\"Evaluate gene-level signal distortion\")\n",
    "    real_data_DE = extract_de_results(adata=adata, batch_key=\"batch_all_with_condition\", cell_type_key=cell_type_key,key_added=\"DE_wilcoxon\")\n",
    "    denoised_DE = extract_de_results(adata=integrated, batch_key=\"batch_all_with_condition\", cell_type_key=cell_type_key,key_added=\"DE_wilcoxon\")\n",
    "    real_data_DE.to_csv(\"./evaluate/\"+dataset_name+\"_real_DE.csv\")\n",
    "    denoised_DE.to_csv(\"./evaluate/\"+dataset_name+\"_denoised_DE.csv\")\n",
    "    print(\"Finish\")\n",
    "    \n",
    "    print(dataset_name, \"Batch-related mixing performance evaluation for main effect:\")\n",
    "    for batch_keyi in batch_key+[condition_key]:\n",
    "        print(dataset_name, \"Batch-related mixing performance evaluation for main effect:\")\n",
    "        print(batch_keyi)\n",
    "        import rpy2.robjects as robjects\n",
    "        import anndata2ri\n",
    "        anndata2ri.activate()\n",
    "        # Add your library path\n",
    "        library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "        # Update R's library paths\n",
    "        robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "        # Verify the updated library paths\n",
    "        print(robjects.r('.libPaths()'))\n",
    "        calculate_metrics(main_effect_adata, batch_key=batch_keyi,\n",
    "                          celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "    print(dataset_name, \"Within each unique condition, plot main effect\")\n",
    "    unique_conditions = np.unique(np.array(integrated.obs[condition_key]))\n",
    "    for conditioni in unique_conditions:\n",
    "        print(dataset_name, \"Now evaluating\", conditioni, \"main effect\")\n",
    "        main_effect_adatai = main_effect_adata[main_effect_adata.obs[condition_key] == conditioni]\n",
    "        sc.pl.umap(main_effect_adatai, color=[cell_type_key] + batch_key, ncols=1)\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    print(dataset_name, \"NMI, ARI for condition in denoised expression\")\n",
    "    import rpy2.robjects as robjects\n",
    "    import anndata2ri\n",
    "    anndata2ri.activate()\n",
    "    # Add your library path\n",
    "    library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "    # Update R's library paths\n",
    "    robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "    # Verify the updated library paths\n",
    "    print(robjects.r('.libPaths()'))\n",
    "    calculate_metrics(integrated, batch_key=\"batch_all\",\n",
    "                      celltype_key=condition_key, all=True, n_neighbors=15)\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    print(dataset_name, \"Within each unique condition, plot denoised expression\")\n",
    "    unique_conditions = np.unique(np.array(integrated.obs[condition_key]))\n",
    "    for conditioni in unique_conditions:\n",
    "        print(dataset_name, \"Now evaluating\", conditioni, \"denoised expression\")\n",
    "        integratedi = integrated[integrated.obs[condition_key] == conditioni]\n",
    "        sc.pl.umap(integratedi, color=[cell_type_key] + batch_key, ncols=1)\n",
    "        import rpy2.robjects as robjects\n",
    "        import anndata2ri\n",
    "        anndata2ri.activate()\n",
    "        # Add your library path\n",
    "        library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "        # Update R's library paths\n",
    "        robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "        # Verify the updated library paths\n",
    "        print(robjects.r('.libPaths()'))\n",
    "        calculate_metrics(integratedi, batch_key=\"batch_all\",\n",
    "                          celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        print(\"*\" * 20)\n",
    "\n",
    "    print(dataset_name, \"Within each unique condition, plot ITE\")\n",
    "    unique_conditions = np.unique(np.array(integrated.obs[condition_key]))\n",
    "    for conditioni in unique_conditions:\n",
    "        print(dataset_name, \"Now evaluating\", conditioni, \"ITE\")\n",
    "        if np.sum(ITE.obs[condition_key] == conditioni) == 0:\n",
    "            continue\n",
    "        ITEi = ITE[ITE.obs[condition_key] == conditioni]\n",
    "        print(ITEi.shape)\n",
    "        sc.pl.umap(ITEi, color=[cell_type_key] + batch_key, ncols=1)\n",
    "\n",
    "        for batch_keyi in batch_key + [condition_key]:\n",
    "            print(dataset_name, \"Batch-related mixing performance evaluation for ITE:\")\n",
    "            print(dataset_name, conditioni,batch_keyi)\n",
    "            import rpy2.robjects as robjects\n",
    "            import anndata2ri\n",
    "            anndata2ri.activate()\n",
    "            # Add your library path\n",
    "            library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "            # Update R's library paths\n",
    "            robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "            # Verify the updated library paths\n",
    "            print(robjects.r('.libPaths()'))\n",
    "            calculate_metrics(ITEi, batch_key=batch_keyi,\n",
    "                              celltype_key=cell_type_key, all=True, n_neighbors=15)\n",
    "        print(\"+\" * 20)\n",
    "\n",
    "    print(\"Condition-related signal preserve evaluation using neighborhood proportion analysis:\")\n",
    "    print(\"NMI, ARI for condition\")\n",
    "    import rpy2.robjects as robjects\n",
    "    import anndata2ri\n",
    "    anndata2ri.activate()\n",
    "    # Add your library path\n",
    "    library_path = \"/gpfs/gibbs/project/wang_zuoheng/xx244/R/4.3/\"  # Replace with the actual path\n",
    "    # Update R's library paths\n",
    "    robjects.r(f'.libPaths(c(\"{library_path}\", .libPaths()))')\n",
    "    # Verify the updated library paths\n",
    "    print(robjects.r('.libPaths()'))\n",
    "    calculate_metrics(integrated, batch_key=\"batch_all\",\n",
    "                      celltype_key=condition_key, all=True, n_neighbors=15)\n",
    "    print(\"-\"*20)\n",
    "    res = cnova.utils.calc_oobNN(integrated, batch_key=\"batch_all_with_condition\", condition_key=condition_key)\n",
    "    df = res.obsm['knn_prop']\n",
    "    df['condition'] = res.obs[condition_key]\n",
    "    df.index.name = \"index\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = pd.melt(df, id_vars=['index', 'condition'], var_name='neighbor', value_name='proportion')\n",
    "    df = df.rename(columns={'index': 'obs_name'})\n",
    "    df.to_csv(\"./evaluate/\" + dataset_name + \"_oobNN.csv\")\n",
    "\n",
    "    g = sea.FacetGrid(df, col='neighbor', hue='condition')\n",
    "    g.map(sea.kdeplot, 'proportion', bw_adjust=2, alpha=1)\n",
    "    g.set(xlabel='NN proportion', ylabel='Density')\n",
    "    g.add_legend()\n",
    "    plt.suptitle('NDreamer integration')\n",
    "    sea.set_style('white')\n",
    "    plt.show()\n",
    "\n",
    "    df_summarize = calculate_mean_proportion_matrix(df)\n",
    "    print(df_summarize)\n",
    "    df_summarize.to_csv(\"./evaluate/\" + dataset_name + \"_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42f7b25-12d9-4642-a3d3-17cce65d8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[\"ECCITE_perturbation\",\"ECCITE\",\"ECCITE_perturbation_nobatch\",\"ASD01\",\"ASD01_nobatch\",\"ASD101\",\"ASD101_nobatch\"]\n",
    "cell_type_keys=[\"Phase\",\"Phase\",\"Phase\",\"CellType\",\"CellType\",\"CellType\",\"CellType\"]\n",
    "batch_keys=[\"replicate\",\"replicate\",\"replicate\",\"Batch\",\"Batch\",\"Batch\",\"Batch\"]\n",
    "condition_keys=[\"perturbation\",\"perturbation\",\"perturbation\",\"perturb01\",\"perturb01\",\"perturb01\",\"perturb01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13ae4c-c6dc-4b7e-a8c5-7e99b9b020d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_names)):\n",
    "    dataset_name = dataset_names[i]\n",
    "    print(\"Evaluating\",dataset_name)\n",
    "    evaluate_ndreamer_ITE_batch(dataset_name=dataset_name,cell_type_key=cell_type_keys[i],batch_key=batch_keys[i],condition_key=condition_keys[i])\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34071d9-0025-4c29-81dc-822e58f9a437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
